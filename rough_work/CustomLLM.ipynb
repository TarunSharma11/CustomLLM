{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7130ac70-85a0-4b3f-9d86-7ab28401d3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import time\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from dataclasses import dataclass\n",
    "import tiktoken\n",
    "import inspect\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "345d4989-6721-45bd-9b9d-c0e954e5f978",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int = 50304\n",
    "    block_size: int = 1024\n",
    "    n_head: int = 12\n",
    "    n_layer: int = 12\n",
    "    n_embd: int = 768\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a095b577-085a-434a-a6c3-3ce6f1e7e3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, config.n_embd * 3)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.c_proj = nn.Linear(self.n_embd, self.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "        self.register_buffer('bias', torch.tril(torch.ones(config.block_size, config.block_size)).view(1,1,config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.size() #x.shape B,T,C\n",
    "        qkv = self.c_attn(x) # B,T, C*3\n",
    "        q, k, v = qkv.split(self.n_embd, dim = 2)\n",
    "        \n",
    "        q = q.view(B,T, self.n_head, C // self.n_head).transpose(1, 2) # B, n_head, T, C\n",
    "        k = k.view(B,T, self.n_head, C // self.n_head).transpose(1, 2) # B, n_head, T, C\n",
    "        v = v.view(B,T, self.n_head, C // self.n_head).transpose(1, 2) # B, n_head, T, C\n",
    "        # attn_weights = (q @ k.transpose(-2, -1)) * (1.0/math.sqrt(q.shape[-1]))\n",
    "        # attn_weights = attn_weights.masked_fill(self.bias[:,:, :T, :T] == 0, float('-inf'))\n",
    "        # attn_weights = F.softmax(attn_weights, dim = -1)\n",
    "\n",
    "        # attn_out = attn_weights @ v\n",
    "        attn_out = F.scaled_dot_product_attention(q, k, v, is_causal = True)\n",
    "        \n",
    "        attn_out = attn_out.transpose(1,2).contiguous().view(B,T,C)\n",
    "        proj_out = self.c_proj(attn_out)\n",
    "        return proj_out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f611a0b-25d7-48e1-b183-a567000e5d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4*config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate = 'tanh')\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x.shape is B,T,C\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c333e49f-c4e4-4680-8603-5a6d2264a1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x.shape is B,T,C\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe0e08af-f8fd-479a-aeba-517735bf04b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd)\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias = False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        self.apply(self._init_weight)\n",
    "\n",
    "    def _init_weight(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
    "                torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02*(2*(self.config.n_layer**-0.5)))\n",
    "            else:\n",
    "                torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02)\n",
    "    \n",
    "    def forward(self, idx, targets = None):\n",
    "        #idx is of shape B,T\n",
    "        T = idx.shape[1] if idx.shape[1] <= self.config.block_size else self.config.block_size\n",
    "        word_embd = self.transformer.wte(idx)     \n",
    "        pos_embd = self.transformer.wpe(torch.arange(T, dtype = torch.long, device = idx.device))\n",
    "        x = word_embd + pos_embd\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    def configure_optimizer(self, model, learning_rate, weight_decay, device):\n",
    "        param_dict = {name: param for name, param in model.named_parameters()}\n",
    "        trainable_param_dict = {name: param for name, param in model.named_parameters() if param.requires_grad}\n",
    "\n",
    "        decay_params = [param for name, param in trainable_param_dict.items() if param.dim() >= 2]\n",
    "        non_decay_params = [param for name, param in trainable_param_dict.items() if param.dim()   < 2]\n",
    "        optim_groups = [\n",
    "            {\n",
    "                \"params\": decay_params,\n",
    "                \"weight_decay\": weight_decay\n",
    "            },\n",
    "            {\n",
    "                \"params\": non_decay_params,\n",
    "                \"weight_decay\": 0.0\n",
    "            }\n",
    "        ]\n",
    "        total_decayed_params = sum([p.numel() for p in decay_params])\n",
    "        total_non_decayed_params = sum([p.numel() for p in non_decay_params])\n",
    "        print(f\"Decayed parameters : {total_decayed_params} | Non decayed params: {total_non_decayed_params}\")\n",
    "        contains_fused = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        is_cuda = device == 'cuda'\n",
    "        fused = contains_fused and is_cuda\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr = learning_rate, betas = (0.9, 0.95), eps = 1e-8, fused = fused)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7d6c11f-feda-4bbe-ab75-06e020270aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(100)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b264c62e-6cbc-4271-9582-7f7646fa3090",
   "metadata": {},
   "outputs": [],
   "source": [
    "#min(self.B, math.ceil((end_index-self.curr_start)/self.T))\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, rank, world_size, num_shards, split = 'train'):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        self.shard_idx = 1\n",
    "        self.num_shards = num_shards\n",
    "        self.file = f\"./data/fineweb_10BT/shard_{split}\"\n",
    "        self.enc_text = self.load_shard(self.shard_idx)\n",
    "        self.curr_start = self.calculate_initial_start()\n",
    "\n",
    "    def calculate_initial_start(self):\n",
    "        return self.B * self.T * self.rank\n",
    "\n",
    "    def load_shard(self, shard_idx):\n",
    "        return np.load(self.file + str(shard_idx) + \".npy\").astype(np.uint16)\n",
    "    \n",
    "    def next_batch(self):\n",
    "        end_index = self.curr_start + self.B * self.T\n",
    "        data = torch.tensor(self.enc_text[self.curr_start: end_index], dtype = torch.long).view(self.B, self.T)\n",
    "        labels = torch.tensor(self.enc_text[self.curr_start + 1: end_index + 1], dtype = torch.long).view(self.B,self.T)\n",
    "        self.curr_start = self.curr_start + self.B * self.T * self.world_size\n",
    "        if ((self.curr_start + self.B * self.T * self.world_size + 1) > len(self.enc_text)):\n",
    "            self.shard_idx = (self.shard_idx + 1) % self.num_shards\n",
    "            self.enc_text = self.load_shard(self.shard_idx)\n",
    "            self.curr_start = (self.B * self.T) * self.rank\n",
    "        return data, labels\n",
    "\n",
    "    def reset(self):\n",
    "        self.curr_start = self.calculate_initial_start()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b1b8796-4e0d-48da-859c-04872a4162fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "import torch.distributed as dist\n",
    "\n",
    "isDDP = int(os.environ.get(\"RANK\", -1)) != -1\n",
    "if isDDP:\n",
    "    dist.init_process_group(backend = 'nccl')\n",
    "    rank = int(os.environ['RANK'])\n",
    "    local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = 'cuda:' + str(local_rank)\n",
    "    isMaster = rank == 0\n",
    "else:\n",
    "    rank = 0\n",
    "    local_rank = 0\n",
    "    world_size = 1\n",
    "    isMaster = True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89699c39-a27d-40c8-90ea-3859eee1b0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(GPTConfig())\n",
    "model.train()\n",
    "model.to(device)\n",
    "model = torch.compile(model)\n",
    "if isDDP:\n",
    "    model = DDP(model, device_ids = [local_rank])\n",
    "    raw_model = model.module\n",
    "else:\n",
    "    raw_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b0ae4e6-8834-418f-a72c-8f8ccd58dab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 100\n",
    "warmup_steps = 10\n",
    "max_lr = 6e-4\n",
    "min_lr = 0.1 * max_lr\n",
    "def get_lr(epoch):\n",
    "    if(epoch <= warmup_steps):\n",
    "        return max_lr * ep/warmup_steps\n",
    "    if(epoch > max_steps):\n",
    "        return min_lr\n",
    "        \n",
    "    curr_ratio = (epoch - warmup_steps)/(max_steps - warmup_steps)\n",
    "    coeff = 0.5 * (1 + math.cos(math.pi * curr_ratio))\n",
    "    lr = min_lr + coeff * (max_lr - min_lr)\n",
    "    return lr\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6ff8464-125d-452d-91c1-cada9c774520",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_and_propagate_loss(x, y, acc_loss):\n",
    "    with torch.autocast(device_type = device, dtype = torch.bfloat16):\n",
    "        logits, loss = model(x,y)\n",
    "        loss = loss / acc_steps\n",
    "    acc_loss += loss.detach() \n",
    "    loss.backward()\n",
    "    return logits, loss, acc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fbd2f37-86f2-4570-871e-7648d168bfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hellaswag_data_url = 'https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_val.jsonl'\n",
    "hellaswag_data_location = './data/hellaswag/val_data.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28052aa2-9e84-46ea-8ed3-71104234d188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, file):\n",
    "    req = requests.get(url, stream = True)\n",
    "    with open(file, 'wb') as f:\n",
    "        for chunk in req.iter_content(chunk_size = 16):\n",
    "            f.write(chunk)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72c99d2b-35ea-4c76-82c0-a80d442a7d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(hellaswag_data_location):\n",
    "    download(hellaswag_data_url, hellaswag_data_location)\n",
    "\n",
    "with open(hellaswag_data_location, 'r') as f:\n",
    "    hellaswag_val_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9532c21-977d-4c43-b24c-5a9b466894e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensor_example_batch(example):\n",
    "    ctx = example['ctx']\n",
    "    label = example['label']\n",
    "    tokenizer = tiktoken.get_encoding('cl100k_im')\n",
    "    ctx_enc = tokenizer.encode(ctx)\n",
    "    endings_enc = [tokenizer.encode(ending) for ending in  example['endings']]\n",
    "    examples_enc = [ctx_enc + end_enc for end_enc in endings_enc]\n",
    "    max_len = max(len(ex_enc) for ex_enc in examples_enc)\n",
    "    padding_lens = [max_len - len(ex_enc) for ex_enc in examples_enc]\n",
    "    padded_examples_enc = [ex_enc + [0]*pad_len for ex_enc, pad_len in zip(examples_enc, padding_lens)]\n",
    "    examples_tensor = torch.tensor(padded_examples_enc, dtype = torch.long)\n",
    "    \n",
    "    padding_mask = torch.ones((4, max_len))\n",
    "    for i, pad_len in enumerate(padding_lens):\n",
    "        padding_mask[i, -pad_len:] = 0\n",
    "\n",
    "    ending_mask = torch.zeros(4, max_len)\n",
    "    for i, end_len in enumerate([len(end_enc) for end_enc in endings_enc]):\n",
    "        ending_mask[i, len(ctx_enc): len(ctx_enc)+end_len] = 1\n",
    "\n",
    "    return examples_tensor, padding_mask, ending_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3933f53c-f25d-4657-b697-5c536defd48c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 19074 #(10^10 / 2**19 i.e. dataset size/batch size)\n",
    "weight_decay = 0.1\n",
    "B = 64\n",
    "T = 1024\n",
    "total_batch_size = 2**19\n",
    "acc_steps = total_batch_size // (B*T*world_size)\n",
    "train_dataloader = DataLoaderLite(16, 1024, rank, world_size, 9, 'train')\n",
    "val_dataloader = DataLoaderLite(16, 1024, rank, world_size, 9, 'val')\n",
    "model_checkpoints_dir = 'checkpoints/'\n",
    "\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4)\n",
    "optimizer = raw_model.configure_optimizer(6e-4, weight_decay = 0.1)\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "\n",
    "encoder = tiktoken.get_encoding(\"gpt2\")\n",
    "generator_feed_tokens = torch.tensor(encoder.encode(\"Hello, I'm a language model,\"), dtype = torch.long)\n",
    "num_rand_generation = 5\n",
    "generator_feed_tokens = generator_feed_tokens.unsqueeze(0).repeat(num_rand_generation,1).to(device)\n",
    "generator_max_length = 50\n",
    "checkpoint_dir = 'checkpoints/'\n",
    "for ep in range(epochs):\n",
    "    last_step = ep == epochs - 1\n",
    "    if (ep > 0) and (last_step or (ep % 500) == 0):\n",
    "        eval_steps = 10\n",
    "        model.eval()\n",
    "        val_dataloader.reset()\n",
    "        with torch.no_grad():\n",
    "            for val_step in range(eval_steps):\n",
    "                val_x, val_y = val_dataloader.next_batch()\n",
    "                val_x, val_y = val_x.to(device), val_y.to(device)\n",
    "                val_loss_acc = 0.0\n",
    "                with torch.autocast(device_type = device, dtype = torch.bfloat16):\n",
    "                    _, val_loss = model(val_x, val_y)   \n",
    "                    val_loss = val_loss / eval_steps\n",
    "                val_loss_acc += val_loss.detach()\n",
    "            if isDDP:\n",
    "                torch.distributed.all_reduce(val_loss_acc, op = dist.ReduceOp.AVG)\n",
    "\n",
    "        if ((not isDDP) or (isMaster)):\n",
    "            print(f\"Epoch : {ep + 1} | Val Loss: {val_loss_acc.item():.4f}\")\n",
    "\n",
    "    if (last_step or (ep % 500) == 0):\n",
    "        hl_total_correct = 0\n",
    "        hl_total_elem = 0\n",
    "        for i, example in enumerate(hellaswag_val_data):\n",
    "            if ((i - rank) % world_size == 0) :\n",
    "                json_ex = json.loads(example)\n",
    "                example_tensor, padding_mask, ending_mask, label = create_tensor_example_batch(json_ex)\n",
    "                example_tensor, padding_mask, ending_mask = example_tensor.to('cuda'), padding_mask.to('cuda'), ending_mask.to('cuda')\n",
    "                logits,_ = model(example_tensor)\n",
    "                probs = F.softmax(logits, dim = 2)\n",
    "                probs = probs[:, :-1, :].contiguous()\n",
    "                y = example_tensor[:, 1:].contiguous()\n",
    "                ending_mask = ending_mask[:, 1:].contiguous()\n",
    "                probs = probs.view(-1, probs.shape[-1])\n",
    "                y = y.view(-1)\n",
    "                loss = F.cross_entropy(probs, y, reduction = 'none')\n",
    "                loss = loss.view(4, -1)\n",
    "                loss_endings = loss * ending_mask\n",
    "                loss_sum = torch.sum(loss_endings, dim = 1)\n",
    "                loss_avg = loss_sum / torch.sum(ending_mask, dim = 1)\n",
    "                pred_label = torch.argmin(loss_avg).item()\n",
    "                hl_total_correct += int(pred_label == label)\n",
    "                hl_total_elem += 1\n",
    "        if isDDP:\n",
    "            torch.distributed.all_reduce(hl_total_correct, op = dist.ReduceOp.SUM)\n",
    "            torch.distributed.all_reduce(hl_total_elem, op = dist.ReduceOp.SUM)\n",
    "        \n",
    "        if isMaster:\n",
    "            print(f\"Hellaswag accurancy: {(hl_total_correct / hl_total_elem)* 100}\")\n",
    "\n",
    "    if (last_step or (ep % 500) == 0):\n",
    "        generated_tokens = torch.clone(generator_feed_tokens)\n",
    "        for i in range(generator_max_length):\n",
    "            with torch.no_grad():\n",
    "                gen_logits, _ = model(generated_tokens)\n",
    "                gen_logits = gen_logits[:, -1, :] # B, C\n",
    "                gen_probs = F.softmax(gen_logits, dim = -1)\n",
    "                topk_probs, topk_indices = torch.topk(gen_probs, 50, dim = -1) # B,50 B,50\n",
    "                ix = torch.multinomial(topk_probs, 1) #B,1\n",
    "                tokens_ix = torch.gather(topk_indices, 1, ix)\n",
    "                generated_tokens = torch.cat((generated_tokens, tokens_ix), dim = -1)\n",
    "                \n",
    "        for i in range(num_rand_generation):\n",
    "            print(f\"Epoch : {ep + 1} | Rank: {rank} | Generated Text: {encoder.decode(generated_tokens.detach().cpu().numpy()[i])}\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    lr = get_lr(ep+1)\n",
    "    for param in optimizer.param_groups:\n",
    "        param['lr'] = lr\n",
    "    acc_loss = 0.0\n",
    "    total_tokens_processed = 0\n",
    "    for step in range(acc_steps):\n",
    "        x,y = train_dataloader.next_batch()\n",
    "        B,T = x.shape\n",
    "        total_tokens_processed += (B*T)\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        if isDDP and (step != (acc_steps - 1)):\n",
    "            with ddp.no_sync():\n",
    "                logits, loss, acc_loss = calculate_and_propagate_loss(x,y, acc_loss)\n",
    "        else:\n",
    "            logits, loss, acc_loss = calculate_and_propagate_loss(x,y, acc_loss)\n",
    "    if isDDP:\n",
    "        torch.distributed.all_reduce(acc_loss, op = dist.ReduceOp.AVG)\n",
    "        torch.distributed.all_reduce(total_tokens_processed, op = dist.ReduceOp.SUM)\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    dt = (t1 - t0) * 1000\n",
    "    tk_sec = total_tokens_processed/(t1 - t0)\n",
    "    if ((not isDDP) or (isMaster)):\n",
    "        print(f\"Epoch : {ep + 1} | Loss: {acc_loss.item():.4f} | Time taken: {dt:.2f} milli sec | Tokens per sec: {tk_sec:.2f}\")\n",
    "\n",
    "    if (ep > 0) and (last_step or (ep % 5000) == 0):\n",
    "        checkpoint_dict = {\n",
    "            'model_dict': raw_model.state_dict(),\n",
    "            'config': raw_model.config,\n",
    "            'val_loss': val_loss_acc.item(),\n",
    "            'step': ep\n",
    "        }\n",
    "        checkpoint_file_name = checkpoint_dir + f\"model_epoch_{ep:05d}.pt\"\n",
    "        torch.save(checkpoint_dict, checkpoint_file_name) \n",
    "    \n",
    "    if isDDP:\n",
    "        destroy_process_group()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a849d554-4c31-46b8-8064-c5fa18352315",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT2",
   "language": "python",
   "name": "gpt2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
